---
title: "Session 9: Scraping Interactive Web Pages (part 2)"
subtitle: "Introduction to Web Scraping and Data Management for Social Scientists"
author: "Johannes B. Gruber"
date: 2024-08-01
from: markdown+emoji
format:
  revealjs:
    smaller: true
    width: 1600
    height: 900
    scrollable: true
    code-line-numbers: true
    slide-number: c/t
    logo: ../ess_logo.png
    embed-resources: true
bibliography: ../references.bib
execute:
  eval: true
  cache: false
  echo: true
engine: knitr
highlight-style: nord
---

# Browser automation
## What is Browser Automation?

```{r setup}
#| echo: false
#| message: false
library(tidyverse)
library(httr2)
library(rvest)
```

- **Definition**: the process of using software to control web browsers and interact with web elements programmatically
- **Tools Involved**: Common tools include `Selenium`, `Puppeteer`, and `Playwright`
- These tools allow scripts to perform actions like clicking, typing, and navigating through web pages automatically

## Common Uses of Browser Automation

- **Testing**: Widely used in software development for automated testing of web applications to ensure they perform as expected across different environments and browsers
- **Task Automation**: Simplifies repetitive tasks such as form submissions, account setups, or any routine that can be standardized across web interfaces

## Browser Automation in Web Scraping

- **Dynamic Content Handling**: Essential for scraping websites that load content dynamically with JavaScript. Automation tools can interact with the webpage, wait for content to load, and then scrape the data.
- **Simulation of User Interaction**: Can mimic human browsing patterns to interact with elements (like dropdowns, sliders, etc.) that need to be manipulated to access data
- **Avoiding Detection**: More sophisticated than basic scraping scripts, browser automation can help mimic human-like interactions, reducing the risk of being detected and blocked by anti-scraping technologies

# Example: Google Maps
## Goal

:::: {.columns}
::: {.column width="50%"}
1. Check the communte time programatically
2. Extract the distance and time it takes to make a journey
:::
::: {.column width="50%" }
[![](media/maps.png)](https://www.google.de/maps/dir/Armadale+St,+Glasgow,+UK/Lilybank+House,+Glasgow,+UK/@55.8626667,-4.2712892,14z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x48884155c8eadf03:0x8f0f8905398fcf2!2m2!1d-4.2163615!2d55.8616765!1m5!1m1!1s0x488845cddf3cffdb:0x7648f9416130bcd5!2m2!1d-4.2904601!2d55.8740368!3e0?entry=ttu){target="_blank"}
:::
::::

## Can we use `rvest`?

:::: {.columns}

::: {.column width="45%"}
```{r}
static <- read_html("https://www.google.de/maps/dir/Armadale+St,+Glasgow,+UK/Lilybank+House,+Glasgow,+UK/@55.8626667,-4.2712892,14z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x48884155c8eadf03:0x8f0f8905398fcf2!2m2!1d-4.2163615!2d55.8616765!1m5!1m1!1s0x488845cddf3cffdb:0x7648f9416130bcd5!2m2!1d-4.2904601!2d55.8740368!3e0?entry=ttu")
static |> 
  html_elements(".Fk3sm") |> 
  html_text2()
```
:::

::: {.column width="50%" }

![google maps commute](media/maps.png)
:::
::::


## Let's use browser Automation

:::: {.columns}

::: {.column width="45%"}
The new `read_html_live` from `rvest` solves this by emulating a browser:

```{r}
# loads a real web browser
sess <- read_html_live("https://www.google.de/maps/dir/Armadale+St,+Glasgow,+UK/Lilybank+House,+Glasgow,+UK/@55.8626667,-4.2712892,14z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x48884155c8eadf03:0x8f0f8905398fcf2!2m2!1d-4.2163615!2d55.8616765!1m5!1m1!1s0x488845cddf3cffdb:0x7648f9416130bcd5!2m2!1d-4.2904601!2d55.8740368!3e0?entry=ttu")
```

You can have a look at the browser with:

```{r}
#| eval: false
sess$view()
```
:::

::: {.column width="50%" }
![](media/chromote.png)

Unfortunately, we do not get content yet. We first have to click on "Accept all"
:::
::::

## Let's use browser Automation

After manipulating something about the session, you need to read it into `R` again:

```{r}
sess <- read_html_live("https://www.google.de/maps/dir/Armadale+St,+Glasgow,+UK/Lilybank+House,+Glasgow,+UK/@55.8626667,-4.2712892,14z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x48884155c8eadf03:0x8f0f8905398fcf2!2m2!1d-4.2163615!2d55.8616765!1m5!1m1!1s0x488845cddf3cffdb:0x7648f9416130bcd5!2m2!1d-4.2904601!2d55.8740368!3e0?entry=ttu")
```

```{r loadcookies}
#| echo: false
cookies <- readRDS("data/chromote_cookies.rds")
sess <- sess$session$Network$setCookies(cookies = cookies$cookies)
sess <- read_html_live("https://www.google.de/maps/dir/Armadale+St,+Glasgow,+UK/Lilybank+House,+Glasgow,+UK/@55.8626667,-4.2712892,14z/data=!3m1!4b1!4m14!4m13!1m5!1m1!1s0x48884155c8eadf03:0x8f0f8905398fcf2!2m2!1d-4.2163615!2d55.8616765!1m5!1m1!1s0x488845cddf3cffdb:0x7648f9416130bcd5!2m2!1d-4.2904601!2d55.8740368!3e0?entry=ttu")
```

Then we can extract information:

```{r}
# the session behaves like a normal rvest html object
trip <- sess |> 
  html_elements("#section-directions-trip-0")

trip |> 
  html_element("h1") |> 
  html_text2()

trip |> 
  html_element(".fontHeadlineSmall") |> 
  html_text2()


trip |> 
  html_element(".fontBodyMedium") |> 
  html_text2()
```


## Store the cookies

- Now that we have accepted the cookie banner, a small set of cookies was stored in the browser
- These are destroyed when we close R, however
- We can extract them and save the session for the next run, so no manual intervention is neccesary


```{r}
#| eval: false
cookies <- sess$session$Network$getCookies()
saveRDS(cookies, "data/chromote_cookies.rds")
```

In the next run, you can load the cookies with:

```{r}
#| eval: false
cookies <- readRDS("data/chromote_cookies.rds")
sess <- sess$session$Network$setCookies(cookies = cookies$cookies)
```


# Example: Reddit
## Goal

:::: {.columns}
::: {.column width="50%"}
1. Get Reddit posts
2. Get the timestamp, up/downvot score, post and comment text
:::
::: {.column width="50%" }
[![](media/reddit.png)](https://www.reddit.com/r/dataviz/){target="_blank"}
:::
::::

## Can we use `rvest` to get post URLs?

```{r}
html_subreddit <- read_html("https://www.reddit.com/r/wallstreetbets/")

posts <- html_subreddit |> 
  html_elements("a") |> 
  html_attr("href") |> 
  str_subset("/comments/") |> 
  str_replace_all("^/", "https://www.reddit.com/") |> 
  unique()
posts
```

:::{.fragment}
This does not look too bad!
:::

:::{.fragment}
Although we only get 3 posts :thinking:
:::

## Can we use `rvest` to get posts?

```{r}
html_post <- read_html("https://www.reddit.com/r/wallstreetbets/comments/1ehcsoq/daily_discussion_thread_for_august_01_2024/")
post_data <- html_post |> 
  html_elements("shreddit-post")

post_data |> 
  html_attr("created-timestamp") |> 
  lubridate::as_datetime()

post_data |> 
  html_attr("id")

post_data |> 
  html_attr("subreddit-id")

post_data |> 
  html_attr("score")

post_data |> 
  html_attr("comment-count")
```

:::{.fragment}
We actually can!
:::

## Can we use `rvest` to get comments?

```{r}
comments <- html_post |> 
  html_elements("shreddit-comment")
comments
```

:::{.fragment}
:expressionless:
:::

## How about `read_html_live`?

```{r}
html_post_live <- read_html_live("https://www.reddit.com/r/wallstreetbets/comments/1ehcsoq/daily_discussion_thread_for_august_01_2024/")
comments <- html_post_live |> 
  html_elements("shreddit-comment")
comments
```

:grin:

:::{.fragment}
But again, something is missing :thinking:
:::

## Interacting with the session

```{r}
#| eval: false
html_post_live$view()
```

Scrolling down as far as possible:

```{r}
html_post_live$scroll_to(top = 10 ^ 5)
```

This triggers new content to be loaded:

```{r}
comments <- html_post_live |> 
  html_elements("shreddit-comment")
comments
```

# Automate the scrolling

```{r}
#| eval: false
last_y <- -1
#scroll as far as possible
while (html_post_live$get_scroll_position()$y > last_y) {
  last_y <- html_post_live$get_scroll_position()$y
  html_post_live$scroll_to(top = 10 ^ 5)
  load_more <- html_post_live |> 
    html_element("[noun=\"load_more_comments\"]") |> 
    length()
  if (load_more) {
    html_post_live$click("[noun=\"load_more_comments\"]")
    html_post_live$scroll_to(top = 10 ^ 5)
  }
  Sys.sleep(1 * runif(1, 1, 3))
}
```



# Alternative: Playwright
## Introducing Playwright

![](media/playwright.png)

- Tool for web testing
- Testing a website and scraping it is actually quite similar
- It essentially uses a special version of a web browser that can be controlled through code from different languages
- Unfortunately no `R` package that wraps the API yet (but and `R` package that wraps the Python package)
- Alternatives you might have heard of: Selenium and Puppeteer

## First, install it

We want to use `playwrightr`, which is an `R` package to control the Python package for Playwright.
So we need 3 pieces for this:

1. The `R` package: install it with `remotes::install_github("JBGruber/playwrightr")`
2. The Python package: we install this into a virtual environment using `reticulate`
3. The Playwright executable, which consists of a modified version of Chrome that can be remote controlled

All three steps are done when you run the code below:

```{r}
if (!rlang::is_installed("playwrightr")) remotes::install_github("JBGruber/playwrightr")
if (!reticulate::virtualenv_exists("r-playwright")) {
  reticulate::virtualenv_install("r-playwright", packages = "playwright")
  playwright_bin <- reticulate::virtualenv_python("r-playwright") |> 
    stringr::str_replace("python", "playwright")
  system(paste(playwright_bin, "install chromium"))
}
reticulate::use_virtualenv("r-playwright")
```


## Control Playwright from `r` with an experimental package

I did not write the package, but made some changes to make it easier to use.

To get started, we first initialize the underlying Python package and then launch Chromium:

```{r}
library(reticulate)
library(playwrightr)
pw_init()
chrome <- browser_launch(
  browser = "chromium", 
  headless = !interactive(), 
  # make sure data like cookies are stored between sessions
  user_data_dir = "user_data_dir/"
)
```

Now we can navigate to a page:

```{r}
page <- new_page(chrome)
goto(page, "https://www.facebook.com/groups/911542605899621")
```

![](media/fb_1.png)

When you are in Europe, the page asks for consent to save cookies in your browser:

![](media/fb_1b.png)

```{r sleep}
#| echo: false
Sys.sleep(3)
```


## Getting more posts

This page loads new content when you scroll down.
We can do this using the `scroll` function:

```{r}
scroll(page)
```

![](media/fb_2.png)

## Getting the page content

Okay, we now see the content.
But what about collecting it?
We can use several different `get_*` functions to identify specfic elements.
But wen can also simply get the entire HTML content:

```{r}
html <- get_content(page)
html
```

Conveniently, this is already an `rvest` object.
So we can use our familiar tools to get to the links of the visible posts.
The page uses a role attribute which Iemploy here and I know that links to posts contain `posts`:

```{r}
post_links <- html |> 
  html_elements("[role=\"link\"]") |> 
  html_attr("href") |> 
  str_subset("posts")
head(post_links)
```

## Collecting Post content

Now we can visit the page of one of these posts and collect the content from it:

```{r}
#| eval: false
post1 <- new_page(chrome)
# go to the page
goto(post1, post_links[1])
post1_html <- get_content(post1)
```

We can check the content we collected locally:

```{r}
#| eval: false
check_in_browser <- function(html) {
  tmp <- tempfile(fileext = ".html")
  writeLines(as.character(html), tmp)
  browseURL(tmp)
}
check_in_browser(post1_html)
```

## Scraping the content: failure

The site uses a lot of weird classes, Making it almost impossible to get content:

```{r}
#| eval: false
author <- post1_html |> 
  html_elements("h2") |> 
  html_text2() |> 
  head(1)

text <- post1_html |> 
  html_element("[id=\":r1j:\"]") |> 
  html_text2()

tibble(author, post_time, text)
```

Nevertheless, some success...

So now that we have the content, we can close the page:

```{r}
#| eval: false
close_page(post1)
```

## What is cool about Playwright

```{r}
#| eval: false
playwright_bin <- reticulate::virtualenv_python("r-playwright") |> 
  stringr::str_replace("python", "playwright")
system(paste(playwright_bin, "codegen"))
```

![](media/codegen.png)

## This can produce Python scripts

```{python}
#| eval: false
import playwright
def run(playwright: Playwright) -> None:
    browser = playwright.chromium.launch(headless=False)
    context = browser.new_context()
    page = context.new_page()
    page.goto("https://www.google.com/")
    page.get_by_role("button", name="Accept all").click()
    page.get_by_label("Search", exact=True).click()
    page.get_by_label("Search", exact=True).fill("amsterdam bijstand")
    page.get_by_role("link", name="Bijstandsuitkering Gemeente").click()

    # ---------------------
    context.close()
    browser.close()


with sync_playwright() as playwright:
    run(playwright)
```


# Summary: Browser Automation
## What is it

- remote control a browser to perform pre-defined steps
- several available tools: 
  - native `R`: `chromote`, used in `read_html_live`
  - native `Python`: `Playwright`, can be used from `R` with `playwrightr` (experimental)
  - native `Java`: `Selenium`, can be used from `R` with  `RSelenium` (buggy and outdated)
  - native `JavaScript`: `Puppeteer` not `R` bindings

## What are they good for?


:::: {.columns}
::: {.column width="60%"}
- Get content from pages which you can't otherwise access
- Load more content through automated scrolling on dynamic pages
- Automate tasks like downloading files
:::
::: {.column width="40%" }
![](https://raw.githubusercontent.com/JBGruber/traktok/main/man/figures/logo.png)
:::

::::

## Issues

:::: {.columns}

::: {.column width="40%" .incremental}
- Companies have mechanisms to counter scraping:
  - rate limiting requests per second/minute/day and user/IP(Twitter)
  - captchas (can be solved but quite complex)
- Won't get you around very obscure HTML code (Facebook)
- Quite heavy and very slow compared to requests
:::

::: {.column width="60%"}
![](https://hackernoon.com/hn-images/0*MPt2rectMhwklT63.jpg)
:::
::::



# Wrap Up

Save some information about the session for reproducibility.

```{r}
sessionInfo()
```
